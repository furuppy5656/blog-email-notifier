import requests
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import json
import os
from datetime import datetime
import re

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
TARGET_URL = os.environ.get('TARGET_URL', 'https://example.com')
TARGET_NAME = os.environ.get('TARGET_NAME', 'ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ')
EMAIL = os.environ.get('EMAIL_ADDRESS')
PASSWORD = os.environ.get('EMAIL_PASSWORD')
CACHE_FILE = 'last_articles.json'

# é™¤å¤–ã™ã‚‹å®šå‹æ–‡ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºï¼‰
EXCLUDE_TEXTS = os.environ.get('EXCLUDE_TEXTS', '').split('|') if os.environ.get('EXCLUDE_TEXTS') else []

def should_exclude(text):
    """é™¤å¤–ã™ã¹ããƒ†ã‚­ã‚¹ãƒˆã‹åˆ¤å®š"""
    for exclude in EXCLUDE_TEXTS:
        if exclude and exclude in text:
            return True
    return False

def clean_content(content):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´å½¢"""
    if not content:
        return ""
    # é™¤å¤–ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰Šé™¤
    for exclude in EXCLUDE_TEXTS:
        if exclude:
            content = content.replace(exclude, "")
    # é€£ç¶šã™ã‚‹ç©ºç™½ã‚„æ”¹è¡Œã‚’æ•´ç†
    content = re.sub(r'\n\s*\n', '\n', content)
    content = re.sub(r'\s+', ' ', content).strip()
    return content

def get_web_content():
    """ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
    try:
        print(f"Fetching content from: {TARGET_URL}")
        response = requests.get(TARGET_URL, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        response.encoding = response.apparent_encoding or 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        articles = []
        
        # æ±ç”¨çš„ãªè¨˜äº‹æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        article_selectors = [
            ('article', {}),
            ('div', {'class': ['post', 'entry', 'article', 'content-item']}),
            ('section', {'class': ['post', 'article']}),
            ('li', {'class': ['post', 'article-item']})
        ]
        
        article_elements = []
        for tag, attrs in article_selectors:
            if attrs:
                found = soup.find_all(tag, attrs)
            else:
                found = soup.find_all(tag)
            if found:
                article_elements.extend(found[:5])
                break
        
        for element in article_elements[:10]:  # æœ€å¤§10ä»¶ã¾ã§
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
            title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'a'])
            if title_elem:
                title = title_elem.get_text().strip()
                
                # é™¤å¤–ãƒ†ã‚­ã‚¹ãƒˆã¯ç„¡è¦–
                if should_exclude(title) or not title:
                    continue
                
                # æ—¥ä»˜ã‚’æ¢ã™ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼‰
                date_text = ""
                date_patterns = [
                    ('time', {}),
                    ('span', {'class': ['date', 'post-date', 'entry-date', 'published']}),
                    ('div', {'class': ['date', 'post-date', 'meta-date']}),
                    ('p', {'class': ['date', 'post-meta']})
                ]
                
                for tag, attrs in date_patterns:
                    date_elem = element.find(tag, attrs)
                    if date_elem:
                        date_text = date_elem.get_text().strip()
                        break
                
                # æœ¬æ–‡ã‚’å–å¾—
                content = ""
                content_patterns = [
                    ('div', {'class': ['content', 'entry-content', 'post-content', 'excerpt']}),
                    ('p', {}),
                    ('span', {'class': ['summary', 'description']})
                ]
                
                for tag, attrs in content_patterns:
                    content_elem = element.find(tag, attrs)
                    if content_elem:
                        content = clean_content(content_elem.get_text())
                        if content:
                            break
                
                # ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                link = ""
                link_elem = element.find('a', href=True)
                if link_elem:
                    link = link_elem['href']
                    if not link.startswith('http'):
                        base_url = TARGET_URL.rstrip('/')
                        link = base_url + '/' + link.lstrip('/')
                
                if title and not title.startswith(('Menu', 'Navigation', 'Category', 'Tags', 'Archives')):
                    articles.append({
                        'title': title,
                        'date': date_text,
                        'content': content[:300] + '...' if len(content) > 300 else content,
                        'link': link or TARGET_URL,
                        'time': datetime.now().isoformat()
                    })
        
        # è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€h2/h3ã‚¿ã‚°ã‚’ç›´æ¥æ¢ã™
        if not articles:
            headings = soup.find_all(['h2', 'h3'])[:10]
            for heading in headings:
                title = heading.get_text().strip()
                if should_exclude(title):
                    continue
                if title and not title.lower().startswith(('menu', 'nav', 'category', 'tag', 'archive', 'search')):
                    # è¦ªè¦ç´ ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    parent = heading.parent
                    link = ""
                    if parent:
                        link_elem = parent.find('a', href=True)
                        if link_elem:
                            link = link_elem['href']
                            if not link.startswith('http'):
                                link = TARGET_URL.rstrip('/') + '/' + link.lstrip('/')
                    
                    articles.append({
                        'title': title,
                        'date': '',
                        'content': '',
                        'link': link or TARGET_URL,
                        'time': datetime.now().isoformat()
                    })
        
        print(f"Found {len(articles)} items")
        return articles
        
    except Exception as e:
        print(f"Error fetching content: {e}")
        return []

def load_cache():
    """å‰å›ãƒã‚§ãƒƒã‚¯æ™‚ã®è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []

def save_cache(articles):
    """è¨˜äº‹æƒ…å ±ã‚’ä¿å­˜"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Error saving cache: {e}")

def send_email(new_articles):
    """æ–°ç€è¨˜äº‹ã‚’ãƒ¡ãƒ¼ãƒ«é€ä¿¡ï¼ˆHTMLå½¢å¼ãƒ»ç›®æ¬¡ä»˜ãï¼‰"""
    if not new_articles:
        return
    
    # HTMLå½¢å¼ã®ãƒ¡ãƒ¼ãƒ«æœ¬æ–‡ä½œæˆ
    html_body = f"""
    <html>
      <head>
        <style>
          body {{ font-family: 'ãƒ¡ã‚¤ãƒªã‚ª', 'Hiragino Sans', sans-serif; line-height: 1.8; color: #333; max-width: 800px; margin: 0 auto; }}
          h1 {{ color: #2c3e50; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }}
          h2 {{ color: #34495e; margin-top: 40px; border-bottom: 2px solid #4CAF50; padding-bottom: 5px; }}
          
          .toc {{ background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px; margin: 20px 0; }}
          .toc h3 {{ color: #495057; margin-top: 0; margin-bottom: 20px; }}
          .toc-item {{ margin-bottom: 25px; padding-bottom: 20px; border-bottom: 1px solid #e9ecef; }}
          .toc-item:last-child {{ border-bottom: none; }}
          .toc-title {{ font-size: 16px; font-weight: bold; color: #2c3e50; text-decoration: none; display: block; margin-bottom: 5px; }}
          .toc-title:hover {{ color: #4CAF50; text-decoration: underline; }}
          .toc-date {{ color: #6c757d; font-size: 14px; margin-bottom: 5px; }}
          .toc-excerpt {{ color: #495057; font-size: 14px; line-height: 1.5; margin-bottom: 8px; }}
          .toc-link {{ color: #3498db; text-decoration: none; font-size: 14px; }}
          .toc-link:hover {{ text-decoration: underline; }}
          
          .article {{ margin: 40px 0; padding: 25px; background-color: #ffffff; border-left: 4px solid #4CAF50; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
          .article-title {{ font-size: 20px; font-weight: bold; color: #2c3e50; margin-bottom: 10px; }}
          .article-date {{ color: #7f8c8d; font-size: 14px; margin-bottom: 15px; }}
          .article-content {{ color: #495057; line-height: 1.8; }}
          .article-link {{ margin-top: 15px; padding-top: 15px; border-top: 1px solid #e9ecef; }}
          .article-link a {{ color: #3498db; text-decoration: none; font-weight: 500; }}
          .article-link a:hover {{ text-decoration: underline; }}
          
          hr {{ border: none; border-top: 1px solid #dee2e6; margin: 40px 0; }}
        </style>
      </head>
      <body>
        <h1>ğŸ”” {TARGET_NAME}ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸï¼</h1>
        
        <!-- ç›®æ¬¡ -->
        <div class="toc">
          <h3>ğŸ“‹ æ–°ç€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¸€è¦§</h3>
    """
    
    # ç›®æ¬¡ã‚’ä½œæˆ
    for i, article in enumerate(new_articles, 1):
        article_id = f"article-{i}"
        html_body += f"""
          <div class="toc-item">
            <a href="#{article_id}" class="toc-title">{article['title']}</a>
        """
        
        if article.get('date'):
            html_body += f'<div class="toc-date">ğŸ“… {article["date"]}</div>'
        
        if article.get('content'):
            excerpt = article['content'][:150] + "..." if len(article['content']) > 150 else article['content']
            html_body += f'<div class="toc-excerpt">{excerpt}</div>'
        
        if article.get('link'):
            html_body += f'<a href="{article["link"]}" class="toc-link">â†’ è¨˜äº‹ã‚’èª­ã‚€</a>'
        
        html_body += "</div>"
    
    html_body += """
        </div>
        
        <hr>
        
        <h2>ğŸ“ è©³ç´°å†…å®¹</h2>
    """
    
    # å„è¨˜äº‹ã®æœ¬æ–‡
    for i, article in enumerate(new_articles, 1):
        article_id = f"article-{i}"
        html_body += f"""
        <div class="article" id="{article_id}">
          <div class="article-title">{article['title']}</div>
        """
        
        if article.get('date'):
            html_body += f'<div class="article-date">ğŸ“… {article["date"]}</div>'
        
        if article.get('content'):
            html_body += f'<div class="article-content">{article["content"]}</div>'
        
        if article.get('link'):
            html_body += f'''
            <div class="article-link">
              <a href="{article["link"]}">ğŸ“– å…¨æ–‡ã‚’èª­ã‚€ â†’</a>
            </div>
            '''
        
        html_body += "</div>"
    
    html_body += f"""
        <hr>
        <p style="text-align: center; color: #6c757d; font-size: 14px;">
          ç›£è¦–å¯¾è±¡URL: <a href="{TARGET_URL}" style="color: #3498db; text-decoration: none;">{TARGET_URL}</a><br>
          ã“ã®ãƒ¡ãƒ¼ãƒ«ã¯è‡ªå‹•é€ä¿¡ã•ã‚Œã¦ã„ã¾ã™
        </p>
      </body>
    </html>
    """
    
    # ãƒ†ã‚­ã‚¹ãƒˆç‰ˆã‚‚ä½œæˆ
    text_body = f"{TARGET_NAME}ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸï¼\n\n"
    text_body += "="*50 + "\n\n"
    
    for i, article in enumerate(new_articles, 1):
        text_body += f"ã€{i}ã€‘ {article['title']}\n"
        if article.get('date'):
            text_body += f"æ—¥ä»˜: {article['date']}\n"
        if article.get('content'):
            text_body += f"{article['content'][:200]}...\n"
        if article.get('link'):
            text_body += f"URL: {article['link']}\n"
        text_body += "\n" + "-"*50 + "\n\n"
    
    text_body += f"URL: {TARGET_URL}"
    
    # ãƒ¡ãƒ¼ãƒ«è¨­å®š
    msg = MIMEMultipart('alternative')
    msg['From'] = EMAIL
    msg['To'] = EMAIL
    msg['Subject'] = f"ã€æ›´æ–°é€šçŸ¥ã€‘{TARGET_NAME} - æ–°ç€ {len(new_articles)}ä»¶"
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒˆã¨HTMLãƒ‘ãƒ¼ãƒˆã‚’è¿½åŠ 
    part1 = MIMEText(text_body, 'plain', 'utf-8')
    part2 = MIMEText(html_body, 'html', 'utf-8')
    
    msg.attach(part1)
    msg.attach(part2)
    
    # Gmailé€ä¿¡
    try:
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(EMAIL, PASSWORD)
            server.send_message(msg)
        print(f"Email sent successfully: {len(new_articles)} new items")
    except Exception as e:
        print(f"Error sending email: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("="*50)
    print(f"Website Monitor Started - {datetime.now()}")
    print(f"Target: {TARGET_URL}")
    print("="*50)
    
    # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
    current_articles = get_web_content()
    if not current_articles:
        print("No content found or error occurred")
        return
    
    # å‰å›ã®è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿
    cached_articles = load_cache()
    cached_titles = {a['title'] for a in cached_articles}
    
    # æ–°ç€è¨˜äº‹ã‚’åˆ¤å®š
    new_articles = [a for a in current_articles if a['title'] not in cached_titles]
    
    if new_articles:
        print(f"New content found: {len(new_articles)} items")
        for article in new_articles:
            print(f"  - {article['title']}")
        send_email(new_articles)
    else:
        print("No new content")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
    save_cache(current_articles)
    print("Process completed")
    print("="*50)

if __name__ == "__main__":
    main()
